_target_: dynamics_toolbox.models.pl_models.sequential_models.transformers.gpt.GPT
num_blocks: 4
embed_dim_per_head: 16
n_heads: 4
block_size: 200
logvar_lower_bound: -10
logvar_upper_bound: 0.5
posn_embedding:
  _target_: dynamics_toolbox.utils.pytorch.modules.positional_embeddings.LearnedEmbedding
  embed_dim: 64
  max_len: 200
shortcut_encoding:
    _target_: dynamics_toolbox.models.pl_models.mlp.MLP
    output_dim: 64
    num_layers: 1
    layer_size: 256
    hidden_activation: "relu"
dim_name_map:
  - "Rewards"
  - "X Position"
  - "Y Position"
  - "Angular Velocity"
